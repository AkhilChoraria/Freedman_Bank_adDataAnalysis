{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the pre-trained model's tokenizer (a.k.a dictionary)\n",
    "model_names = {'base uncased': 'bert-base-uncased', 'large uncased': 'bert-large-uncased', \n",
    "               'base cased': 'bert-base-cased', 'large cased': 'bert-large-cased'}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1865010212209: | (['[CLS]', 'open', 'daily', 'from', '1', 'to', '5', 'pm', 'wed', '##es', '##day', '##s', 'and', 'saturdays', 'from', '1', 'to', '7', 'o', \"'\", 'clock', '[SEP]', 'six', 'per', 'cent', 'interest', 'allowed', '[SEP]', 'the', 'january', 'divide', '##nd', 'will', 'be', 'paid', 'free', 'from', 'gov', '##en', '##ment', 'tax', '[SEP]', 'money', 'deposits', 'on', 'or', 'before', 'jan', '[SEP]', '10', 'will', 'draw', 'interest', 'from', 'jan', '[SEP]', '1', '[SEP]'], [101, 2330, 3679, 2013, 1015, 2000, 1019, 7610, 21981, 2229, 10259, 2015, 1998, 18860, 2013, 1015, 2000, 1021, 1051, 1005, 5119, 102, 2416, 2566, 9358, 3037, 3039, 102, 1996, 2254, 11443, 4859, 2097, 2022, 3825, 2489, 2013, 18079, 2368, 3672, 4171, 102, 2769, 10042, 2006, 2030, 2077, 5553, 102, 2184, 2097, 4009, 3037, 2013, 5553, 102, 1015, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5])\n",
      "\n",
      "\n",
      "1865010212203: | (['[CLS]', 'deposits', 'will', 'draw', 'interest', 'from', 'the', '1st', 'january', '[SEP]', 'bank', 'open', 'daily', 'from', '10', 'am', 'to', '2', 'pm', 'and', 'on', 'monday', ',', 'wed', '##es', '##day', 'and', 'saturday', 'evenings', ',', 'from', '5', 'to', '7', 'o', \"'\", 'clock', '[SEP]'], [101, 10042, 2097, 4009, 3037, 2013, 1996, 3083, 2254, 102, 2924, 2330, 3679, 2013, 2184, 2572, 2000, 1016, 7610, 1998, 2006, 6928, 1010, 21981, 2229, 10259, 1998, 5095, 16241, 1010, 2013, 1019, 2000, 1021, 1051, 1005, 5119, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "\n",
      "1865010212216: | (['[CLS]', 'the', 'subscription', 'to', 'this', 'gold', 'interest', 'loan', 'will', 'be', 'open', 'for', 'this', 'week', 'at', 'the', 'fiscal', 'agency', 'of', 'the', 'u', '[SEP]', 's', '[SEP]', 'treasury', ',', 'the', 'ninth', '##h', 'national', 'bank', '[SEP]', 'all', 'five', 'per', 'cent', 'interest', 'bearing', 'notes', 'of', 'the', 'united', 'states', 'received', 'in', 'payment', ',', 'and', 'the', 'acc', '##ured', 'interest', 'allowed', '[SEP]'], [101, 1996, 15002, 2000, 2023, 2751, 3037, 5414, 2097, 2022, 2330, 2005, 2023, 2733, 2012, 1996, 10807, 4034, 1997, 1996, 1057, 102, 1055, 102, 9837, 1010, 1996, 6619, 2232, 2120, 2924, 102, 2035, 2274, 2566, 9358, 3037, 7682, 3964, 1997, 1996, 2142, 2163, 2363, 1999, 7909, 1010, 1998, 1996, 16222, 12165, 3037, 3039, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Scripts import createTokensFromAd\n",
    "from Scripts import data_wrapper\n",
    "ads_tokenized_text = createTokensFromAd.main()\n",
    "tokenized_ads = {}\n",
    "\n",
    "for identifier, marked_text in ads_tokenized_text.items():\n",
    "    tokenized_ads[identifier] = data_wrapper.Tokenized()\n",
    "    tokenized_ads[identifier].text = tokenizer.tokenize(marked_text)\n",
    "    tokenized_ads[identifier].index = tokenizer.convert_tokens_to_ids(tokenized_ads[identifier].text)\n",
    "    tokenized_ads[identifier].segments = createTokensFromAd.getSegmentIDs(tokenized_ads[identifier].text)\n",
    "\n",
    "\n",
    "# TODO: Test that tokenized_ads is accurately generated.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5f9e2482a693c8b32e66cdc5103ae302a60b2aeedba5852f87dfe7756d28f23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
