{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 59.6kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 11.5MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 7.45MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 1.18MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the pre-trained model's tokenizer (a.k.a dictionary)\n",
    "model_names = {'base uncased': 'bert-base-uncased', 'large uncased': 'bert-large-uncased', \n",
    "               'base cased': 'bert-base-cased', 'large cased': 'bert-large-cased'}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'director', 'of', 'the', 'metropolitan', 'bank', 'have', 'this', 'day', 'declared', 'a', 'semi', '-', 'annual', 'divide', '##nd', 'of', 'five', 'per', 'cent', '(', 'free', 'of', 'tax', ')', ',', 'pay', '##able', 'after', '1st', 'monday', 'of', 'january', 'next', '[SEP]', 'the', 'transfer', 'books', 'to', 'remain', 'closed', 'from', 'dec', '[SEP]', '17', 'until', 'jan', '[SEP]', '1965', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from Scripts import createTokensFromAd\n",
    "ads_tokenized_text = createTokensFromAd.main()\n",
    "tokenized_ads={}\n",
    "for identifier, marked_text in ads_tokenized_text.items():\n",
    "    tokenized_ads[identifier]=tokenizer.tokenize(marked_text)\n",
    "    \n",
    "#print(tokenized_ads[\"1865010212211\"])\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5f9e2482a693c8b32e66cdc5103ae302a60b2aeedba5852f87dfe7756d28f23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
